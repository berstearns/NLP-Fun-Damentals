{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "punct = string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# mps_device = torch.device(\"mps\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df = pd.read_csv('/Users/gauneg/datasets/fun_dsets/sourth_park_ds.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions:\n",
    "1. `context_ngrams`: creates all (CONTEXT, WORD) pairs for input sentence\n",
    "2. `flat_input_gen`: Applies context_ngrams to entire input dataset\n",
    "3. `create_batch`: batches inputs for learning\n",
    "4. `separate_x_y_encode`: encodes batched sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def context_ngrams(word_arr, context_size, mode=\"uni_dir\"):\n",
    "    txt = []\n",
    "    end_point = len(word_arr) - context_size if mode==\"bi_dir\" else len(word_arr)\n",
    "    for i in range(context_size, end_point):\n",
    "        if mode==\"uni_dir\":\n",
    "            txt.append([word_arr[i-context_size: i], word_arr[i]])\n",
    "        else:\n",
    "            txt.append([word_arr[i-context_size: i] +  word_arr[i+1: i + context_size+1], \n",
    "                        word_arr[i]])\n",
    "    return txt\n",
    "\n",
    "def flat_input_gen(inp_text_list, context_function):\n",
    "    flat_arr = []\n",
    "    for sentence in inp_text_list:\n",
    "        sent_no_punct = \"\".join([alpha for alpha in sentence if alpha not in punct])\n",
    "        tokenized_sentence = word_tokenize(sent_no_punct) #.split()\n",
    "        tokenized_sentence = [lemmatizer.lemmatize(token) for token in tokenized_sentence]\n",
    "        for context_outs in context_function(tokenized_sentence):\n",
    "            flat_arr.append(context_outs)\n",
    "    return flat_arr\n",
    "\n",
    "def create_batch(flat_arr, batch_size):\n",
    "    fin_index = len(flat_arr) - len(flat_arr)%batch_size\n",
    "\n",
    "    for cur_index in range(0, fin_index, batch_size):\n",
    "        \n",
    "        yield flat_arr[cur_index: cur_index+batch_size]\n",
    "\n",
    "    if fin_index<=len(flat_arr):\n",
    "        yield flat_arr[fin_index:]\n",
    "\n",
    "def separate_x_y_encode(xy_comb, word_to_ix):\n",
    "    ctx = []\n",
    "    words = []\n",
    "    for context, word in xy_comb:\n",
    "        ctx.append([word_to_ix.get(kx, word_to_ix['<unk>']) for kx in context])\n",
    "        words.append(word_to_ix.get(word, word_to_ix['<unk>']))\n",
    "    return torch.tensor(ctx), torch.tensor(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDINGS_DIM = 10\n",
    "cartman_df = df[df['Character']=='Cartman']\n",
    "cartman_lines = [text.strip().lower() for text in cartman_df['Line'].values]\n",
    "crt_dry = cartman_lines\n",
    "\n",
    "part_applied_context = lambda txt_arr: context_ngrams(txt_arr, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select vocabulary \n",
    "\n",
    "X = flat_input_gen(crt_dry, part_applied_context)\n",
    "vocab = [ai for a,b in X for ai in a]\n",
    "word_freq = {}\n",
    "\n",
    "for word in vocab:\n",
    "    if word.lower() not in word_freq.keys():\n",
    "        word_freq[word.lower()] = 0\n",
    "    word_freq[word.lower()] += 1\n",
    "\n",
    "word_frx = [(word, cx) for word, cx in word_freq.items()]\n",
    "sort_vocab = sorted(word_frx, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_to_ix = {x[0]: i+1 for i, x in enumerate(sort_vocab[:1600])}\n",
    "word_to_ix['<unk>'] = 0\n",
    "ix_to_word = {x: k for k,x in word_to_ix.items()}\n",
    "\n",
    "batched_inps = list(create_batch(X, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModelerBatched(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size) -> None:\n",
    "        super(NGramLanguageModelerBatched, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_size = context_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.liner1 = nn.Linear(context_size * embedding_dim, 128, )\n",
    "        self.liner2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((-1, self.embedding_dim*self.context_size))\n",
    "        # print('Embedding output', embeds.shape)\\\n",
    "        out = F.relu(self.liner1(embeds))\n",
    "        \n",
    "        out = self.liner2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramLanguageModelerBatched(\n",
       "  (embeddings): Embedding(1601, 10)\n",
       "  (liner1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (liner2): Linear(in_features=128, out_features=1601, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModelerBatched(len(word_to_ix), EMBEDDINGS_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model#.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(250):\n",
    "    total_loss = 0\n",
    "    for batch in batched_inps:\n",
    "        enc_x, enc_y = separate_x_y_encode(batch, word_to_ix)\n",
    "        log_probs = model(enc_x)\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(log_probs, enc_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(batched_inps):\n",
    "        if i>2:\n",
    "            enc_x, enc_y = separate_x_y_encode(batch, word_to_ix)\n",
    "            log_probs = model(enc_x)\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why', 'do'] we\n",
      "['do', 'you'] think\n",
      "['you', 'need'] to\n",
      "['kenny', '<unk>'] <unk>\n",
      "['<unk>', '<unk>'] <unk>\n",
      "['<unk>', 'nut'] <unk>\n",
      "['a', '<unk>'] <unk>\n",
      "['a', '<unk>'] <unk>\n",
      "['maybe', 'maybe'] we\n",
      "['maybe', 'he'] is\n",
      "['he', 'still'] me\n",
      "['still', 'okay'] <unk>\n",
      "['okay', 'no'] more\n",
      "['no', 'really'] <unk>\n",
      "['really', 'they'] have\n",
      "['they', 'say'] that\n",
      "['say', 'the'] <unk>\n",
      "['the', 'last'] time\n",
      "['last', 'thing'] would\n",
      "['thing', 'you'] guy\n",
      "['you', 'do'] we\n",
      "['do', 'before'] he\n",
      "['before', 'you'] guy\n",
      "['you', 'die'] <unk>\n",
      "['die', 'is'] a\n",
      "['is', 'crap'] a\n",
      "['oh', 'never'] know\n",
      "['hey', 'everybody'] is\n",
      "['everybody', 'there'] are\n",
      "['there', 'more'] time\n",
      "['more', 'pop'] <unk>\n",
      "['pop', 'in'] <unk>\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    log_ax = log_probs.argmax(dim=-1)\n",
    "    print([ix_to_word[ax.item()] for ax in enc_x[i]], ix_to_word[log_ax[i].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
