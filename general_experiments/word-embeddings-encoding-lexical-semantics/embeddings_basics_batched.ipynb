{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "punct = string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df = pd.read_csv('/Users/gauneg/datasets/fun_dsets/sourth_park_ds.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions:\n",
    "1. `context_ngrams`: creates all (CONTEXT, WORD) pairs for input sentence\n",
    "2. `flat_input_gen`: Applies context_ngrams to entire input dataset\n",
    "3. `create_batch`: batches inputs for learning\n",
    "4. `separate_x_y_encode`: encodes batched sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def context_ngrams(word_arr, context_size, mode=\"uni_dir\"):\n",
    "    txt = []\n",
    "    end_point = len(word_arr) - context_size if mode==\"bi_dir\" else len(word_arr)\n",
    "    for i in range(context_size, end_point):\n",
    "        if mode==\"uni_dir\":\n",
    "            txt.append([word_arr[i-context_size: i], word_arr[i]])\n",
    "        else:\n",
    "            txt.append([word_arr[i-context_size: i] +  word_arr[i+1: i + context_size+1], \n",
    "                        word_arr[i]])\n",
    "    return txt\n",
    "\n",
    "def flat_input_gen(inp_text_list, context_function):\n",
    "    flat_arr = []\n",
    "    for sentence in inp_text_list:\n",
    "        sent_no_punct = \"\".join([alpha for alpha in sentence if alpha not in punct])\n",
    "        tokenized_sentence = word_tokenize(sent_no_punct) #.split()\n",
    "        tokenized_sentence = [lemmatizer.lemmatize(token) for token in tokenized_sentence]\n",
    "        for context_outs in context_function(tokenized_sentence):\n",
    "            flat_arr.append(context_outs)\n",
    "    return flat_arr\n",
    "\n",
    "def create_batch(flat_arr, batch_size):\n",
    "    fin_index = len(flat_arr) - len(flat_arr)%batch_size\n",
    "\n",
    "    for cur_index in range(0, fin_index, batch_size):\n",
    "        \n",
    "        yield flat_arr[cur_index: cur_index+batch_size]\n",
    "\n",
    "    if fin_index<=len(flat_arr):\n",
    "        yield flat_arr[fin_index:]\n",
    "\n",
    "def separate_x_y_encode(xy_comb, word_to_ix):\n",
    "    ctx = []\n",
    "    words = []\n",
    "    for context, word in xy_comb:\n",
    "        ctx.append([word_to_ix.get(kx, word_to_ix['<unk>']) for kx in context])\n",
    "        words.append(word_to_ix.get(word, word_to_ix['<unk>']))\n",
    "    return torch.tensor(ctx), torch.tensor(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDINGS_DIM = 10\n",
    "cartman_df = df[df['Character']=='Cartman']\n",
    "cartman_lines = [text.strip().lower() for text in cartman_df['Line'].values]\n",
    "crt_dry = cartman_lines\n",
    "\n",
    "part_applied_context = lambda txt_arr: context_ngrams(txt_arr, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select vocabulary \n",
    "\n",
    "X = flat_input_gen(crt_dry, part_applied_context)\n",
    "vocab = [ai for a,b in X for ai in a]\n",
    "word_freq = {}\n",
    "\n",
    "for word in vocab:\n",
    "    if word.lower() not in word_freq.keys():\n",
    "        word_freq[word.lower()] = 0\n",
    "    word_freq[word.lower()] += 1\n",
    "\n",
    "word_frx = [(word, cx) for word, cx in word_freq.items()]\n",
    "sort_vocab = sorted(word_frx, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_to_ix = {x[0]: i+1 for i, x in enumerate(sort_vocab[:1600])}\n",
    "word_to_ix['<unk>'] = 0\n",
    "ix_to_word = {x: k for k,x in word_to_ix.items()}\n",
    "\n",
    "batched_inps = list(create_batch(X, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModelerBatched(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size) -> None:\n",
    "        super(NGramLanguageModelerBatched, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_size = context_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.liner1 = nn.Linear(context_size * embedding_dim, 128, )\n",
    "        self.liner2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((-1, self.embedding_dim*self.context_size))\n",
    "        # print('Embedding output', embeds.shape)\\\n",
    "        out = F.relu(self.liner1(embeds))\n",
    "        \n",
    "        out = self.liner2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModelerBatched(len(word_to_ix), EMBEDDINGS_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22949.436532497406\n",
      "20674.334274291992\n",
      "19984.43032836914\n",
      "19652.853738069534\n",
      "19443.432136058807\n",
      "19288.52066373825\n",
      "19163.700829982758\n",
      "19057.56549358368\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for batch in batched_inps:\n",
    "        enc_x, enc_y = separate_x_y_encode(batch, word_to_ix)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(enc_x)\n",
    "        loss = loss_function(log_probs, enc_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
